{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239f9778-14fc-4776-a065-6a7bdd40bd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f8f5ac-e403-489a-b350-ec3094ecb443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "df = dg.Datasets(spark, \"basic/user\").get(rows=1000_000).build()\n",
    "num_rows=df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad524021-e4d8-4a75-af38-db95c695696a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d265a0a-16cb-4855-809f-1e61b72ab4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS magnusp_catalog.training.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f426996-9d07-4f35-b99e-86882c5786ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"/Volumes/magnusp_catalog/training/source/user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56d3ba8-0c8b-4e8b-922c-eccf77691f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# clear cache so that if we run multiple times to check performance,\n",
    "\n",
    "\n",
    "UNIQUE_PLANS = 20\n",
    "PLAN_MIN_VALUE = 100\n",
    "\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 1\n",
    "data_rows = UNIQUE_PLANS # we'll generate one row for each plan\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "plan_dataspec = (\n",
    "    dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "    .withColumn(\"plan_id\",\"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS)\n",
    "    # use plan_id as root value\n",
    "    .withColumn(\"plan_name\", prefix=\"plan\", baseColumn=\"plan_id\")\n",
    "\n",
    "    # note default step is 1 so you must specify a step for small number ranges,\n",
    "    .withColumn(\"cost_per_mb\", \"decimal(5,3)\", minValue=0.005, maxValue=0.050,\n",
    "                step=0.005, random=True)\n",
    "    .withColumn(\"cost_per_message\", \"decimal(5,3)\", minValue=0.001, maxValue=0.02,\n",
    "                step=0.001, random=True)\n",
    "    .withColumn(\"cost_per_minute\", \"decimal(5,3)\", minValue=0.001, maxValue=0.01,\n",
    "                step=0.001, random=True)\n",
    "\n",
    "    # we're modelling long distance and international prices simplistically -\n",
    "    # each is a multiplier thats applied to base rate\n",
    "    .withColumn(\"ld_multiplier\", \"decimal(5,3)\", minValue=1.5, maxValue=3, step=0.05,\n",
    "                random=True, distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"ld_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * ld_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'ld_multiplier'])\n",
    "    .withColumn(\"intl_multiplier\", \"decimal(5,3)\", minValue=2, maxValue=4, step=0.05,\n",
    "                random=True,  distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"intl_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * intl_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'intl_multiplier'])\n",
    "            )\n",
    "\n",
    "df_plans = plan_dataspec.build().cache()\n",
    "\n",
    "display(df_plans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727d1baa-585c-43c5-8050-eb8e18ab8d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_plans.write.format(\"json\").mode(\"overwrite\").save(\"/Volumes/magnusp_catalog/training/source/plans.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebd4b29-4bd2-4b4f-96ef-4e6c455ef780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = spark.read.format(\"json\").load(\"/Volumes/magnusp_catalog/training/source/plans.json\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710972c6-ab22-42ea-9cd5-0901d03755c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "UNIQUE_CUSTOMERS = 50000\n",
    "CUSTOMER_MIN_VALUE = 1000\n",
    "DEVICE_MIN_VALUE = 1000000000\n",
    "SUBSCRIBER_NUM_MIN_VALUE = 1000000000\n",
    "\n",
    "spark.catalog.clearCache()  # clear cache so that if we run multiple times to check\n",
    "                            # performance, we're not relying on cache\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "data_rows = UNIQUE_CUSTOMERS\n",
    "\n",
    "customer_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "            .withColumn(\"customer_id\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS)\n",
    "            .withColumn(\"customer_name\", template=r\"\\\\w \\\\w|\\\\w a. \\\\w\")\n",
    "\n",
    "            # use the following for a simple sequence\n",
    "            #.withColumn(\"device_id\",\"decimal(10)\", minValue=DEVICE_MIN_VALUE,\n",
    "            #              uniqueValues=UNIQUE_CUSTOMERS)\n",
    "\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"customer_id\", baseColumnType=\"hash\")\n",
    "\n",
    "            .withColumn(\"phone_number\",\"decimal(10)\",  minValue=SUBSCRIBER_NUM_MIN_VALUE,\n",
    "                        baseColumn=[\"customer_id\", \"customer_name\"], baseColumnType=\"hash\")\n",
    "\n",
    "            # for email, we'll just use the formatted phone number\n",
    "            .withColumn(\"email\",\"string\",  format=\"subscriber_%s@myoperator.com\",\n",
    "                        baseColumn=\"phone_number\")\n",
    "            .withColumn(\"plan\", \"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS,\n",
    "                        random=True)\n",
    "            )\n",
    "\n",
    "df_customers = (customer_dataspec.build()\n",
    "                .dropDuplicates([\"device_id\"])\n",
    "                .dropDuplicates([\"phone_number\"])\n",
    "                .orderBy(\"customer_id\")\n",
    "                .cache()\n",
    "               )\n",
    "\n",
    "effective_customers = df_customers.count()\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b32695-0307-432b-8fc4-c2352aed62a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c813344-adc1-46a3-a8d2-7f934d8b9ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"/Volumes/magnusp_catalog/training/source/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c4d847-49fe-4461-a255-e0a1c19c3bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/magnusp_catalog/training/source/schemas/plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa31413-84b2-4b0c-81c4-83628ac6e082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "AVG_EVENTS_PER_CUSTOMER = 50\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "NUM_DAYS=31\n",
    "MB_100 = 100 * 1000 * 1000\n",
    "K_1 = 1000\n",
    "data_rows = AVG_EVENTS_PER_CUSTOMER * UNIQUE_CUSTOMERS * NUM_DAYS\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "# use random seed method of 'hash_fieldname' for better spread - default in later builds\n",
    "events_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested,\n",
    "                   randomSeed=42, randomSeedMethod=\"hash_fieldname\")\n",
    "             # use same logic as per customers dataset to ensure matching keys\n",
    "             # but make them random\n",
    "            .withColumn(\"device_id_base\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS,\n",
    "                        random=True, omit=True)\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"device_id_base\", baseColumnType=\"hash\")\n",
    "\n",
    "            # use specific random seed to get better spread of values\n",
    "            .withColumn(\"event_type\",\"string\",\n",
    "                        values=[ \"sms\", \"internet\", \"local call\", \"ld call\", \"intl call\" ],\n",
    "                        weights=[50, 50, 20, 10, 5 ], random=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short calls\n",
    "            .withColumn(\"base_minutes\",\"decimal(7,2)\",\n",
    "                        minValue=1.0, maxValue=100.0, step=0.1,\n",
    "                        distribution=dg.distributions.Gamma(shape=1.5, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short transfers\n",
    "            .withColumn(\"base_bytes_transferred\",\"decimal(12)\",\n",
    "                        minValue=K_1, maxValue=MB_100,\n",
    "                        distribution=dg.distributions.Gamma(shape=0.75, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            .withColumn(\"minutes\", \"decimal(7,2)\",\n",
    "                        baseColumn=[\"event_type\", \"base_minutes\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type in (\"local call\", \"ld call\", \"intl call\")\n",
    "                                  then base_minutes\n",
    "                                  else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "            .withColumn(\"bytes_transferred\", \"decimal(12)\",\n",
    "                        baseColumn=[\"event_type\", \"base_bytes_transferred\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type = \"internet\"\n",
    "                                   then base_bytes_transferred\n",
    "                                   else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "\n",
    "            .withColumn(\"event_ts\", \"timestamp\",\n",
    "                         data_range=dg.DateRange(\"2020-07-01 00:00:00\",\n",
    "                                                 \"2020-07-31 11:59:59\",\n",
    "                                                 \"seconds=1\"),\n",
    "                        random=True)\n",
    "\n",
    "            )\n",
    "\n",
    "df_events = events_dataspec.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdcaef2-68c6-4cab-939c-5786d1bba8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events.write.mode(\"overwrite\").saveAsTable(\"magnusp_catalog.training.events\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2507833646509423,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Test data generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
