{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214bd02f-4772-4b08-82bc-4cdb395c3878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test data generation\n",
    "This is what creates the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4801d6a9-fb5f-4c6f-889a-1d4ac95d24ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"magnusp_catalog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239f9778-14fc-4776-a065-6a7bdd40bd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7732fa40-f593-45a7-b85d-4acc3fd53c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"magnusp_catalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d61bcfc-bcb4-4f74-b21a-1ff9890e17ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${catalog_name}.training_test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d265a0a-16cb-4855-809f-1e61b72ab4e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the volume"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Please remember to change catalog to the catalog name you want\n",
    "CREATE SCHEMA IF NOT EXISTS ${catalog_name}.training;\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog_name}.training.source;\n",
    "CREATE SCHEMA IF NOT EXISTS ${catalog_name}.training_raw;\n",
    "CREATE SCHEMA IF NOT EXISTS ${catalog_name}.training_silver;\n",
    "CREATE SCHEMA IF NOT EXISTS ${catalog_name}.training_gold;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56d3ba8-0c8b-4e8b-922c-eccf77691f41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Genrate the plans"
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# clear cache so that if we run multiple times to check performance,\n",
    "\n",
    "\n",
    "UNIQUE_PLANS = 20\n",
    "PLAN_MIN_VALUE = 100\n",
    "\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 1\n",
    "data_rows = UNIQUE_PLANS # we'll generate one row for each plan\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "plan_dataspec = (\n",
    "    dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "    .withColumn(\"plan_id\",\"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS)\n",
    "    # use plan_id as root value\n",
    "    .withColumn(\"plan_name\", prefix=\"plan\", baseColumn=\"plan_id\")\n",
    "\n",
    "    # note default step is 1 so you must specify a step for small number ranges,\n",
    "    .withColumn(\"cost_per_mb\", \"decimal(5,3)\", minValue=0.005, maxValue=0.050,\n",
    "                step=0.005, random=True)\n",
    "    .withColumn(\"cost_per_message\", \"decimal(5,3)\", minValue=0.001, maxValue=0.02,\n",
    "                step=0.001, random=True)\n",
    "    .withColumn(\"cost_per_minute\", \"decimal(5,3)\", minValue=0.001, maxValue=0.01,\n",
    "                step=0.001, random=True)\n",
    "\n",
    "    # we're modelling long distance and international prices simplistically -\n",
    "    # each is a multiplier thats applied to base rate\n",
    "    .withColumn(\"ld_multiplier\", \"decimal(5,3)\", minValue=1.5, maxValue=3, step=0.05,\n",
    "                random=True, distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"ld_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * ld_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'ld_multiplier'])\n",
    "    .withColumn(\"intl_multiplier\", \"decimal(5,3)\", minValue=2, maxValue=4, step=0.05,\n",
    "                random=True,  distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"intl_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * intl_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'intl_multiplier'])\n",
    "            )\n",
    "\n",
    "df_plans = plan_dataspec.build().cache()\n",
    "\n",
    "display(df_plans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727d1baa-585c-43c5-8050-eb8e18ab8d9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Store the plans in the volume"
    }
   },
   "outputs": [],
   "source": [
    "# You have to change this to point to the volume you created\n",
    "volume_path = \"/Volumes/magnusp_catalog/training/source\"\n",
    "df_plans.write.format(\"json\").mode(\"overwrite\").save(f\"{volume_path}/plans.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebd4b29-4bd2-4b4f-96ef-4e6c455ef780",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test read the file created"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = \"/Volumes/magnusp_catalog/training/source\"\n",
    "x = spark.read.format(\"json\").load(f\"{volume_path}/plans.json\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710972c6-ab22-42ea-9cd5-0901d03755c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Customers"
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "UNIQUE_CUSTOMERS = 50000\n",
    "CUSTOMER_MIN_VALUE = 1000\n",
    "DEVICE_MIN_VALUE = 1000000000\n",
    "SUBSCRIBER_NUM_MIN_VALUE = 1000000000\n",
    "\n",
    "spark.catalog.clearCache()  # clear cache so that if we run multiple times to check\n",
    "                            # performance, we're not relying on cache\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "data_rows = UNIQUE_CUSTOMERS\n",
    "\n",
    "customer_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "            .withColumn(\"customer_id\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS)\n",
    "            .withColumn(\"customer_name\", template=r\"\\\\w \\\\w|\\\\w a. \\\\w\")\n",
    "\n",
    "            # use the following for a simple sequence\n",
    "            #.withColumn(\"device_id\",\"decimal(10)\", minValue=DEVICE_MIN_VALUE,\n",
    "            #              uniqueValues=UNIQUE_CUSTOMERS)\n",
    "\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"customer_id\", baseColumnType=\"hash\")\n",
    "\n",
    "            .withColumn(\"phone_number\",\"decimal(10)\",  minValue=SUBSCRIBER_NUM_MIN_VALUE,\n",
    "                        baseColumn=[\"customer_id\", \"customer_name\"], baseColumnType=\"hash\")\n",
    "\n",
    "            # for email, we'll just use the formatted phone number\n",
    "            .withColumn(\"email\",\"string\",  format=\"subscriber_%s@myoperator.com\",\n",
    "                        baseColumn=\"phone_number\")\n",
    "            .withColumn(\"plan\", \"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS,\n",
    "                        random=True)\n",
    "            )\n",
    "\n",
    "df_customers = (customer_dataspec.build()\n",
    "                .dropDuplicates([\"device_id\"])\n",
    "                .dropDuplicates([\"phone_number\"])\n",
    "                .orderBy(\"customer_id\")\n",
    "                .cache()\n",
    "               )\n",
    "\n",
    "effective_customers = df_customers.count()\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c813344-adc1-46a3-a8d2-7f934d8b9ad1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Customers to volume"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = \"/Volumes/magnusp_catalog/training/source\"\n",
    "df_customers.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(f\"{volume_path}/customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c4d847-49fe-4461-a255-e0a1c19c3bbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create folder in volume for schemas"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = \"/Volumes/magnusp_catalog/training/source\"\n",
    "dbutils.fs.mkdirs(f\"{volume_path}/schemas/plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa31413-84b2-4b0c-81c4-83628ac6e082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "AVG_EVENTS_PER_CUSTOMER = 50\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "NUM_DAYS=31\n",
    "MB_100 = 100 * 1000 * 1000\n",
    "K_1 = 1000\n",
    "data_rows = AVG_EVENTS_PER_CUSTOMER * UNIQUE_CUSTOMERS * NUM_DAYS\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "# use random seed method of 'hash_fieldname' for better spread - default in later builds\n",
    "events_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested,\n",
    "                   randomSeed=42, randomSeedMethod=\"hash_fieldname\")\n",
    "             # use same logic as per customers dataset to ensure matching keys\n",
    "             # but make them random\n",
    "            .withColumn(\"device_id_base\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS,\n",
    "                        random=True, omit=True)\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"device_id_base\", baseColumnType=\"hash\")\n",
    "\n",
    "            # use specific random seed to get better spread of values\n",
    "            .withColumn(\"event_type\",\"string\",\n",
    "                        values=[ \"sms\", \"internet\", \"local call\", \"ld call\", \"intl call\" ],\n",
    "                        weights=[50, 50, 20, 10, 5 ], random=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short calls\n",
    "            .withColumn(\"base_minutes\",\"decimal(7,2)\",\n",
    "                        minValue=1.0, maxValue=100.0, step=0.1,\n",
    "                        distribution=dg.distributions.Gamma(shape=1.5, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short transfers\n",
    "            .withColumn(\"base_bytes_transferred\",\"decimal(12)\",\n",
    "                        minValue=K_1, maxValue=MB_100,\n",
    "                        distribution=dg.distributions.Gamma(shape=0.75, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            .withColumn(\"minutes\", \"decimal(7,2)\",\n",
    "                        baseColumn=[\"event_type\", \"base_minutes\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type in (\"local call\", \"ld call\", \"intl call\")\n",
    "                                  then base_minutes\n",
    "                                  else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "            .withColumn(\"bytes_transferred\", \"decimal(12)\",\n",
    "                        baseColumn=[\"event_type\", \"base_bytes_transferred\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type = \"internet\"\n",
    "                                   then base_bytes_transferred\n",
    "                                   else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "\n",
    "            .withColumn(\"event_ts\", \"timestamp\",\n",
    "                         data_range=dg.DateRange(\"2020-07-01 00:00:00\",\n",
    "                                                 \"2020-07-31 11:59:59\",\n",
    "                                                 \"seconds=1\"),\n",
    "                        random=True)\n",
    "\n",
    "            )\n",
    "\n",
    "df_events = events_dataspec.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdcaef2-68c6-4cab-939c-5786d1bba8a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate the events as a table"
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"magnusp_catalog\"\n",
    "df_events.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.training_raw.events\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2507833646549822,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Test data generation",
   "widgets": {
    "catalog_name": {
     "currentValue": "magnusp_catalog",
     "nuid": "e05ef134-323e-4cc4-bb11-e1a6527ffc70",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "magnusp_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "magnusp_catalog",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
