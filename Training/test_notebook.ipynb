{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d6d311-9e8f-4f91-9008-0c3c485f2d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction to this notebook\n",
    "This notebook we created on the fly in the workshop. I did stumble on a few things, and I want to add some explainations what went wrong and provide the corrected result:\n",
    "* \"select * from values({'a': 1, 'b': 2})\"\n",
    "The correct syntax for spark is of course:\n",
    "\"select * from values(map('a',1,'b',2))\", and the reason for the confusion is probably a bit too much programming in other languages where dictionaries has {} as their persisted shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bf33792-894c-40dc-bd65-b969160b59c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table (\n",
    "  name = \"magnusp_catalog.training_raw.my_table_from_test\"\n",
    ")\n",
    "def func():\n",
    "  return spark.readStream.table(\"carpartlifespan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a062af6f-a293-45ee-a142-3847e2c4d55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import array\n",
    "@dlt.table (\n",
    "  name = \"my_table_2\"\n",
    ")\n",
    "def func():\n",
    "  # return array of maps\n",
    "  return spark.sql(\"select * from values(array(map('a',2,'c',1 ), map('b',3,'d',4)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bcb97c5-c9e5-4995-91b8-7e43d6d4f742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We got an error in the session when we tried doing a group by.\n",
    "This is normal if you do it on a dataframe, but not normal if you do it directly on the dataframe. dataframe.max only supports numerical fields, however the function max accepts max of string as with SQL.\n",
    "\n",
    "\n",
    "Below you have the corrected syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e836bf7-d6f8-4e10-9293-23700a026e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "@dlt.view(name=\"max_component\")\n",
    "def func():\n",
    "  df =  spark.readStream.table(\"magnusp_catalog.training_raw.my_table_from_test\")\n",
    "  return df.groupBy().agg(max(\"component\").alias(\"max_component\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
