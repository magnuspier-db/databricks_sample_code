{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c26bf1a-89f2-48e6-897b-edb95b231a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE magnusp_catalog.training;\n",
    "\n",
    "CREATE OR REFRESH STREAMING TABLE CarPartLifespan\n",
    "SCHEDULE REFRESH EVERY 8 WEEK\n",
    "COMMENT 'This table is used to track the lifespan of car parts'\n",
    "TBLPROPERTIES ('pipelines.channel' = 'preview')\n",
    "AS\n",
    "SELECT \n",
    "  Component\n",
    ", Description\n",
    ", `Typical Material` as Material\n",
    ", `Estimated Lifespan` as lifespan\n",
    "from STREAM read_files (\n",
    "  '/Volumes/magnusp_catalog/training/source/*.csv'\n",
    ", format => 'csv'\n",
    ", header => true\n",
    ", mode => 'FAILFAST'\n",
    ")\n",
    "GROUP BY ALL\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744080b1-d6dd-450e-ae12-4e7bd1256337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE magnusp_catalog.training;\n",
    "CREATE OR REFRESH STREAMING TABLE CarPartLifespan3 (\n",
    "  Component STRING,\n",
    "  Description STRING MASK magnusp_catalog.row_col_governance.general_mask_string using columns (\n",
    "    'magnusp_catalog', 'training', 'carpartlifespan3','Description'\n",
    "    ),\n",
    "  Material STRING MASK magnusp_catalog.row_col_governance.general_mask_string using columns (\n",
    "    'magnusp_catalog', 'training', 'carpartlifespan3','Material'\n",
    "    ),\n",
    "  lifespan STRING,\n",
    "  CONSTRAINT CarPartLifespan3_pk PRIMARY KEY (Component, Description,Material) RELY\n",
    ")\n",
    "SCHEDULE REFRESH EVERY 8 WEEK\n",
    "COMMENT 'This table is used to track the lifespan of car parts'\n",
    "TBLPROPERTIES ('pipelines.channel' = 'preview')\n",
    "AS\n",
    "SELECT \n",
    "  Component\n",
    ", Description\n",
    ", `Typical Material` as Material\n",
    ", `Estimated Lifespan` as lifespan\n",
    "from STREAM read_files (\n",
    "  '/Volumes/magnusp_catalog/training/source/*.csv'\n",
    ", format => 'csv'\n",
    ", header => true\n",
    ", mode => 'FAILFAST'\n",
    ")\n",
    "GROUP BY ALL\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4440c06c-5000-4593-9db0-e1fa5b43213e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MATERIALIZED VIEW magnusp_catalog.training.Lifespan_And_Components\n",
    "AS\n",
    "WITH T1 AS (\n",
    "  SELECT\n",
    "    CASE  \n",
    "      WHEN (lifespan) like '% years' THEN 'YEARS'\n",
    "      WHEN (lifespan) like '% months' THEN 'MONTHS'\n",
    "      WHEN lower(lifespan) like '% miles%' THEN 'MILES'\n",
    "      WHEN lower(lifespan) like '% oil change%' THEN 'OIL'\n",
    "      WHEN lower(lifespan) like '%lifetime%' THEN 'VEHICLE'\n",
    "      ELSE 'OTHER'\n",
    "      END as unit\n",
    "  , if (\n",
    "        contains(lower(lifespan), \"-\"), \n",
    "          cast(replace(trim(split(lower(lifespan),\"-\")[0]), \",\", \"\") as INT), \n",
    "        if(\n",
    "            contains(lower(lifespan),\"miles\") = true OR contains(lower(lifespan),\"years\") = true, \n",
    "              cast(replace(trim(split(lower(lifespan),\" \")[0]), \",\", \"\") as INT),\n",
    "            NULL\n",
    "        )) as lifespan_min\n",
    "  ,  if (\n",
    "          contains(lower(lifespan), \"-\"), \n",
    "            cast(replace(split(trim(split(lower(lifespan),\"-\")[1]), \" \")[0], \",\", \"\") as int), NULL\n",
    "    ) as lifespan_max\n",
    "  , array_sort(collect_set(lower(component))) as components \n",
    "  FROM \n",
    "    magnusp_catalog.training.CarPartLifespan3\n",
    "  GROUP BY 1,2,3\n",
    "  ORDER BY 1,2\n",
    ")\n",
    "  SELECT \n",
    "    * except(components)\n",
    "  , posexplode(components) as (component_pos, component) \n",
    "  FROM T1\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df124dd-21e3-4f4f-9bd2-ae1f6a4ebc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE magnusp_catalog.training;\n",
    "CREATE OR REPLACE MATERIALIZED VIEW magnusp_catalog.training.Lifespan_And_Components_V2\n",
    "AS\n",
    "WITH T1 AS (\n",
    "  SELECT\n",
    "    replace (\n",
    "      replace ( \n",
    "        replace (\n",
    "          lifespan, \n",
    "          \" \", \n",
    "          \"<SPC>\"\n",
    "        ), \n",
    "        \"-\", \n",
    "        \"<DASH>\"\n",
    "      ),\n",
    "      \",\", \n",
    "      \"<COMMA>\"\n",
    "    ) as lifespan_split_space\n",
    "  , component\n",
    "  FROM carpartlifespan3\n",
    "),\n",
    "T2 (\n",
    "  SELECT \n",
    "    filter (\n",
    "      flatten (\n",
    "        transform (\n",
    "          split(lifespan_split_space, \"<SPC>\"),  -- INPUT ARRAY\n",
    "            x -> split ( -- FOR EACH ELEMENT IN ARRAY SPLIT\n",
    "              replace (\n",
    "                x, -- FIELD\n",
    "                \"<COMMA>\", -- WHAT TO REPLACE\n",
    "                \"\" -- REPLACE WITH EMPTY\n",
    "              ), \n",
    "              \"<DASH>\" -- FIELD TO SPLIT ON\n",
    "            )\n",
    "        )\n",
    "      ),  \n",
    "      x-> lower(x) NOT IN (\n",
    "        \"\", \n",
    "        \"of\",\n",
    "        \"the\", \n",
    "        \"(belt)\", \n",
    "        \"with\", \n",
    "        \"each\", \n",
    "        \"change\", \n",
    "        \"lifetime\", \n",
    "        \"replace\"\n",
    "      ) \n",
    "    )  as lifespan_split_space \n",
    "  , component\n",
    "  FROM t1\n",
    "), T3 (\n",
    "  SELECT \n",
    "    upper(slice(reverse(lifespan_split_space),1, 1)[0]) as unit\n",
    "  , reverse(slice(reverse(lifespan_split_space),2, size(lifespan_split_space)-1)) as rest\n",
    "  , array_sort(collect_set(lower(component))) as components\n",
    "  FROM T2\n",
    "  GROUP BY 1,2\n",
    ") , T4 (\n",
    "  SELECT \n",
    "    unit\n",
    "  , CAST(rest[0] as INT) as lifespan_min\n",
    "  , CAST(rest[1] AS INT) as lifespan_max \n",
    "  , posexplode(components) (component_pos, component)\n",
    "  FROM T3\n",
    ")\n",
    "SELECT * FROM T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925bf826-660d-41f5-aa4e-bf360801e845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM magnusp_catalog.training.Lifespan_And_Components_V2\n",
    "ORDER BY 1,2,3,4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c867630-3578-4105-b28b-3f565c2ad1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Functions introduced:\n",
    "* posexplode\n",
    "* array_sort\n",
    "* reverse\n",
    "* transform\n",
    "* split\n",
    "* slice\n",
    "* size\n",
    "* contains\n",
    "* filter\n",
    "* collect_set\n",
    "* flatten\n",
    "* cast\n",
    "\n",
    "Summary:\n",
    "Databricks has lots of functions not available in other languages. The reason for this is since we are not working with well prepared sets, we support nested structures, and can process in structure very efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34451a79-8d23-424c-b78d-b4f3d3ce4c0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "POSEXPLODE"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "display(df.select(posexplode(df.intlist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a508adc-c12a-42b4-b48d-cea3f195cede",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "POSEXPLODE OUTER"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import posexplode_outer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
    "    (\"id\", \"an_array\", \"a_map\")\n",
    ")\n",
    "display(df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8716f0b-b5be-4218-98ef-a4941f8c0ee9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ARRAY_SORT"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lit, array_sort, length, when\n",
    "df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
    "df.select(array_sort(df.data).alias('r')).collect()\n",
    "\n",
    "df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n",
    "df2 = df.select(array_sort(\n",
    "    \"data\",\n",
    "    lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n",
    ").alias(\"r\"))\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9a234a-c5c8-488b-b522-ff4ef728fe35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "REVERSE"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import reverse\n",
    "df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
    "display(df.select(reverse(df.data).alias('s')));\n",
    "\n",
    "df = spark.createDataFrame([([2, 1, 3,14],) ,([1,2],) ,([],)], ['data'])\n",
    "display(df.select(reverse(df.data).alias('r')));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fff7f9f-ea9c-4876-aee6-4052b2b2a0f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TRANSFORM"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import transform\n",
    "\n",
    "df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
    "display(df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")))\n",
    "\n",
    "# Alternatively you can define your own python function and pass in:\n",
    "\n",
    "def alternate(x, i):\n",
    "    return when(i % 2 == 0, x).otherwise(-x)\n",
    "\n",
    "display(df.select(transform(\"values\", alternate).alias(\"alternated\")))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caba4d0f-c666-4932-ae3c-1258f38db904",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SPLIT"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import split\n",
    "df = spark.createDataFrame([('oneAtwoBthreeCfour',)], ['s',])\n",
    "display(df.select(split(df.s, '[ABC]', 3).alias('s')))\n",
    "\n",
    "display(df.select(split(df.s, '[ABC]', -1).alias('s')))\n",
    "display(df.select(split(df.s, '[ABC]').alias('s')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d953959c-0ca8-4e8d-a67c-e4ae1a04f935",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SLICE"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import slice\n",
    "\n",
    "df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
    "df2 = df.select(slice(df.x, 2, 2).alias(\"sliced\"))\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d003836-cc12-43b4-acd8-d1a10de9fca6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SIZE"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  ([1, 2, 3],),\n",
    "  ([1],),\n",
    "  ([],),\n",
    "  (None,)],\n",
    "  ['data']\n",
    ")\n",
    "display(df.select(size(df.data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37d3448-0256-4e9f-b029-9520d48c895a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CONTAINS"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import contains\n",
    "df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n",
    "display(df.select(contains(df.a, df.b).alias('r')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2dba48-46ec-4aa5-a3ad-3ec957530a4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FILTER"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import filter, month, to_date\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\", \"2020-09-01\"])],\n",
    "    (\"key\", \"values\")\n",
    ")\n",
    "def after_second_quarter(x):\n",
    "    return month(to_date(x)) > 6\n",
    "\n",
    "df2 = df.select(\n",
    "    filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
    ")\n",
    "\n",
    "display(df2)\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962310ce-92fb-40e3-bf90-42d53c1ac6c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COLLECT_SET"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import collect_set\n",
    "df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
    "display(df2.agg(array_sort(collect_set('age')).alias('c')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698b23a0-500a-4866-a983-faed2d62b1c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FLATTEN"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import flatten, array_compact\n",
    "df = spark.createDataFrame([\n",
    "  ([[1, 2, 3], [4, 5], [6]],), \n",
    "  ([None, [4, 5]],)], ['data'])\n",
    "# Pre processing\n",
    "display(df)\n",
    "\n",
    "# flatten the array into one array\n",
    "display(df.select(flatten(df.data).alias('r')))\n",
    "\n",
    "# array_compact removes null values from array\n",
    "display(df.select(flatten(array_compact(df.data).alias(''))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d304ade-0d90-49e9-b6b3-a8eaf9776984",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Column.Cast"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StringType\n",
    "df = spark.createDataFrame(\n",
    "     [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
    "display(df.select(df.age.cast(\"string\").alias('ages')))\n",
    "\n",
    "display(df.select(df.age.cast(StringType()).alias('ages')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c10daaf-3bf7-49a1-812d-c78229edd0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9603c61c-09ed-44c5-b0d5-2ca81a63345c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01041526-ab53-4321-9c54-6d8ab7ab3c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.sql(\"select * from magnusp_catalog.training.lifespan_and_components_v2\")\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b7f431-ec35-4325-8e6d-9482f469e4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.explain(\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2d078d-70dc-4558-8d82-a9d4f901f8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.explain(\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29f191c-69bc-4158-a50e-4eb80e157cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "x = spark.read.format(\"delta\").load(\"/Volumes/magnusp_catalog/database1/sources/helly_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab710518-f0a4-4105-8dbb-082a3a65683a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Need more work"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "def renormalize(itr) -> Iterator[pd.DataFrame]:\n",
    "    for df in itr:\n",
    "        x = df.explode(\"attribs\")\n",
    "        yield pd.DataFrame( {'name': x['_name'], 'default': x['_default']})\n",
    "            \n",
    "df = x.select(col('xs:complexType.xs:attribute').alias(\"attribs\"))\n",
    "#expected_schema = 'name string, default string'\n",
    "#df = df.mapInPandas(renormalize, expected_schema)\n",
    "\n",
    "pdf = df.toPandas().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d0cdf3-096c-4c81-bc6f-96812df7cf63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JOINS"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "x = spark.read.table(\"magnusp_catalog.training.lifespan_and_components_v2\").alias(\"x\") \n",
    "y = spark.read.table(\"magnusp_catalog.training.lifespan_and_components\").alias(\"y\")\n",
    "\n",
    "x2 = x.subtract(y)\n",
    "display(x2)\n",
    "x3 = y.subtract(x)\n",
    "display(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ec1548-1178-49d6-9ff5-ae4337f8d72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce972a5-8e0f-4b67-98ee-746b5462c28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "x = spark.read.table(\"magnusp_catalog.training.lifespan_and_components_v2\").alias(\"x\") \n",
    "y = spark.read.table(\"magnusp_catalog.training.lifespan_and_components\").alias(\"y\")\n",
    "\n",
    "x2 = x.join(y, [\"unit\", \"component\", \"component_pos\"], \"left_anti\")\n",
    "display(x2)\n",
    "\n",
    "x3 = x.join(y, [\"unit\", \"component\", \"component_pos\"], \"left\")\n",
    "display(x3)\n",
    "\n",
    "x4 = x.join(y, [\"unit\", \"component\", \"component_pos\"], \"right\")\n",
    "display(x4)\n",
    "\n",
    "x5 = x.join(y, [\"unit\", \"component\", \"component_pos\"], \"FULL\")\n",
    "display(x5)\n",
    "x5.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa4e3b5-c794-4a54-b534-bfa57c8b6292",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LATERAL VIEW"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "WITH T1 (\n",
    "SELECT T2,T3,T4POS, T4\n",
    "  LATERAL VIEW       EXPLODE(sequence(1, 100)) as T2\n",
    "  LATERAL VIEW OUTER EXPLODE(shuffle(sequence(1000,1010))) as T3\n",
    "  LATERAL VIEW       POSEXPLODE(shuffle(sequence(1000,1010))) as T4POS,T4\n",
    "ORDER BY 1,2\n",
    ")\n",
    "SELECT * FROM T1 TABLESAMPLE(10 PERCENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4937e67-cd38-4165-b9d0-024355ee7a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select unit,count(*) cnt from magnusp_catalog.training.lifespan_and_components_v2\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9613b289-4746-4a23-9a88-f7254814fa83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "x = spark.read.table(\"magnusp_catalog.training.Lifespan_And_Components_V2\")\n",
    "y = x.repartitionByRange(5,'unit')\n",
    "y.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/magnusp_catalog/database1/sources/partitioned_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce6f7e3-7750-4b0f-8d12-38602cda6e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "\n",
    "y = spark.read.format(\"delta\").load(\"/Volumes/magnusp_catalog/database1/sources/partitioned_set\").select(\"*\",\"_metadata.file_name\")\n",
    "y1 = y.groupBy(\"file_name\").count().orderBy(\"file_name\")\n",
    "display(y1)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Example constructs",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
